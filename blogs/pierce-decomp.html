---
layout: page
permalink: /cool-things/part-3/thoughts/1
---
<div class="content">
	<div class="titleMain">
		<a href="/">Home</a> &gt;
		<a href="#">Cool Things</a> &gt;
		<a href="/cool-things/part-3">Part 3</a> &gt;
		<a href="/cool-things/part-3/thoughts">Blog</a> &gt;
		<a href="#">1</a> &gt;
	</div>
	<h1><span> </span>When its really just a Matrix!</h1>
	<div class="product">
		<a href="#">
			<img alt="me ... coming soon" src="products/prod1.jpg" />
		</a>
	</div>
	
	<p>Pierce decompositions provide a rich collection of tools to the 
		decomposition tool box. Often shrouded by notation and big words
		they answer a rather simple and well motivated 
		question: "Is this thing really just a matrix if I squint hard 
		enough?" and often the answer is yes!</p>
		<p>Lets look at an example of what we are after: Matrices. 
			Let $M_2(\mathbb{R})$ be the $2$-by-$2$ matrices over $\mathbb{R}$ and notice that we can decompose any matrix into into components, meaning
			$$M_2(\mathbb{R})=\left\{\begin{bmatrix}a & b\\ c & d\end{bmatrix} \Big| a,b,c,d\in\mathbb{R} \right\}=\left\{\begin{bmatrix}a & 0\\ 0 & 0\end{bmatrix} \right\}\oplus\left\{\begin{bmatrix}0 & b\\ 0 & 0\end{bmatrix} \right\}\oplus \left\{\begin{bmatrix}0 & 0\\ c & 0\end{bmatrix} \right\}\oplus \left\{\begin{bmatrix}0 & 0\\ 0 & d\end{bmatrix} \right\}$$
		So our motivating question is this: When is an arbitrary Algebra decomposable into what looks like a behaves like the components of a matrix? And when is multiplication really just matrix multiplication?</p>
	<h3>The History</h3>
	<p>ill skip this for now but ill comment that the Krull-Schmidt theorem was first proven by Wedderburn, who doesnt not get their names on the theorem.</p>

	<h3>Why Study Representations?</h3>
	<p>connect to modules. idk give top of triangle of representation (what James calls Cayley's Resonance)</p>
	<h3>Why Decompose Representations?</h3>
	<p>What is a decompose?</p>
	<h3>How to Decompose: A Recursive Algorithm to Decompose Representations</h3>
	<p>Lets first look at an example of a decomposition to see what we should being looking for. 
		Consider the $2$-dimensional vector space $V=K^2$ over your favorite field $K$. This can be 
		thought of as an action by scalar multiplication on $V$, where $K\times V\rightarrow V$. Its easy to see that
		$V$ decomposed into two subspaces on its components: $$K^2=\left\{\begin{bmatrix}*\\0\end{bmatrix}\right\}\oplus \left\{\begin{bmatrix}0\\ *\end{bmatrix}\right\}\cong K\oplus K$$</p>
	<p>This is a rather boring example but it can still help highlight what we are after, in the sense that each of these 
		factors can be thought of as coming from a projection. This idea can be highlighted in following diagram:$$j$$</p>
	<p>This is telling us that in order to find decompositions we need to find idempotents. idk what to put here but we get a correspondence between decompositions of modules and a collection of idempotent maps on $V$</p>
	<p>Theorem: There is a 1-to-1 correspondence between collections of idempotents $\mathcal{E}\subseteq\text{End}(V)$ and direct decompositions of $V$</p>
</div>